# Runing autoencoder training function across multiple VMV blocks in parallel (final training, fine tuning grid search, etc.)

This is a brief set of instructions I gathered from my training runs in DGX and garibaldi nodes. 
This is just a simplified, quick, and generalized example (the quick&dirty way that does the job with minimum effort & maximum benefit haha). Feel free to run your tests your own way, with your own algorithms, programming languages, organization standards, etc.

## Setup

From a chromosome folder containing all VMV files generated by the previous extractor step (*.VMV1 suffix), here is one example of how to create a bash script to run a training queue using 4 GPUs from DGX node (if running in garibaldi, change the number of gpus to 2):

```
#customizable parameters
total_gpus=4 #change this to the total number of gpus that you have in your server,node
root_dir="/raid/chr22" #dirrectory where the VMV1 files are
start_mask=0.80 #starting masking fraction
hp=hyper_parameter_list.txt #hyperparameter file
result="result" #directory to save results

#Non customizable
gpu=0
if [ ! -d $result ];then
    mkdir $result
fi
((gi=total_gpus-1))

for i in $root_dir/*.vcf.VMV1; do

    name=$(basename $i)
    found_error=$(grep -i error result/$name.log | wc -l)
    found_result=$(grep -w RESULT result/$name.out | wc -l)

    if [ $found_error -ne 0 ] || [ $found_result -eq 0 ]; then

        nvar=$(grep -v "#" $i | wc -l)
        nmask=$((nvar-4))
        mrate=$(bc -l <<< "$nmask/$nvar" | sed -e 's/^\./0\./g')

        echo -e "CUDA_VISIBLE_DEVICES=$gpu python3 script.py $i $hp False $start_mask $mrate 1>result/$name.out 2>result/$name.log";

        ((gpu=gpu+1))

        if [ $gpu -gt $gi ]; then

            gpu=0

        fi

    fi

done > to_run_list.sh
```

## Running in DGX

All the commands to run are saved at to_run_list.sh, then you can easily run them in parallel, let's say that we can run 2 models per GPU, and we have 4 gpus in DGX, then the command should look like:

```
nohup parallel -j 8 < to_run_list.sh &
```

If you want to use docker, then you should create a script that will look like this:

```
cat run_docker.sh

opts="--shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864"
subdir=$(basename $(pwd))
pardir=$(pwd | sed -e "s/\/$subdir//g")
echo $subdir $pardir
nvidia-docker run $opts --net=host -v /raid/chr22:/raid/chr22 -v $pardir:/workspace nvcr.io/nvidia/tensorflow:19.07-py3_RD /bin/bash -c "cd $subdir;  parallel -j 8 < run_test.sh"
```

where nvcr.io/nvidia/tensorflow:19.07-py3_RD is the name of my modified docker image, shared through dropbox: https://www.dropbox.com/s/jxre9lru6vq8p5b/tensorflow_19.07-py3_RD_11-21-2019.tar.gz?dl=0
Please refer to docker documentation for more details on how to load the docker image and on the meaning of the input arguments: https://docs.docker.com/engine/reference/commandline/run/

Then you can run:

```
nohup bash run_docker.sh &
```

## Running in Garibaldi nodes

If you want to run the to_run_list.sh commands across several nodes in garibaldi, I recommend to use a bash chunker command and split the to_run_list.sh into 4 command chunks:

```
split -l 4 -a 3 -d --additional-suffix .sh to_run_list.sh to_run_chunk_
```

You will see something like this:

```
ls to_run_chunk*
to_run_chunk_000.sh  to_run_chunk_008.sh  to_run_chunk_016.sh  to_run_chunk_024.sh  to_run_chunk_032.sh  to_run_chunk_040.sh
to_run_chunk_001.sh  to_run_chunk_009.sh  to_run_chunk_017.sh  to_run_chunk_025.sh  to_run_chunk_033.sh  to_run_chunk_041.sh
to_run_chunk_002.sh  to_run_chunk_010.sh  to_run_chunk_018.sh  to_run_chunk_026.sh  to_run_chunk_034.sh  to_run_chunk_042.sh
to_run_chunk_003.sh  to_run_chunk_011.sh  to_run_chunk_019.sh  to_run_chunk_027.sh  to_run_chunk_035.sh  to_run_chunk_043.sh
to_run_chunk_004.sh  to_run_chunk_012.sh  to_run_chunk_020.sh  to_run_chunk_028.sh  to_run_chunk_036.sh  to_run_chunk_044.sh
to_run_chunk_005.sh  to_run_chunk_013.sh  to_run_chunk_021.sh  to_run_chunk_029.sh  to_run_chunk_037.sh  to_run_chunk_045.sh
to_run_chunk_006.sh  to_run_chunk_014.sh  to_run_chunk_022.sh  to_run_chunk_030.sh  to_run_chunk_038.sh  to_run_chunk_046.sh
to_run_chunk_007.sh  to_run_chunk_015.sh  to_run_chunk_023.sh  to_run_chunk_031.sh  to_run_chunk_039.sh
```

Then you can run a batch job script like this:

```
for i in to_run_chunk_*.sh; do 
    qsub submit_gpu_chunk.job -v cmd_lst=$i -N $i; 
done
```

where your job script submit_gpu_chunk.job will have cmd_lst as an input argument that specifies which chunk to run:
```
cat submit_gpu_chunk.job

#!/bin/bash
#PBS -l nodes=1:ppn=16
#PBS -l mem=32gb
#PBS -q gpu
#PBS -l walltime=440:00:00
#PBS -j oe

echo "Running on node:"
hostname
module load tensorflow/1.12.0py36-cuda

#usually there are 2 gpus per node, only 2 nodes have 4 gpus
gpus=2

#How I ran training before
#qsub run_gpu.job -v cmd_lst=/gpfs/home/raqueld/Autoencoder_tensorflow/random_grid_v3/15-19_commands.txt

cd $PBS_O_WORKDIR
parallel -j $gpus :::: $cmd_lst
```

you can also replace the the line:

```
#PBS -l nodes=1:ppn=16
```

in the job script by this one to request good gpus (old ones may fail, so I recommend to use those):

```
#PBS -l nodes=1:ppn=20:gtx1080
```

Or by this one, to request the Titan V GPUs (best ones, but there are only two nodes like this):

```
#PBS -l nodes=1:ppn=16:titan
```

Then delete the line:

```
#PBS -q gpu
```

So the job scheduler will place you in the correct queue automatically based on the gpu resources that you requested.

Have fun!

