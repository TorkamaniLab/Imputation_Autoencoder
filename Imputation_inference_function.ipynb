{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.memory_stats.python.ops.memory_stats_ops import BytesInUse\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# sorting results\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import timeit #measure runtime\n",
    "import random #masking\n",
    "import operator #remove entire columns from 2d arrays\n",
    "\n",
    "import multiprocessing #parallel tasks\n",
    "from functools import partial # pool.map with multiple args\n",
    "\n",
    "\n",
    "\n",
    "do_parallel_MAF = True\n",
    "ncores = multiprocessing.cpu_count() #for parallel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(posfile, infile, categorical=\"False\"):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    #Header and column names start with hastag, skip those\n",
    "    #posfile should contain 2 columns separated by tab: 1st = chromosome ID, 2nd = position\n",
    "    #vcf can be imported as posfile as well, but will take much longer to read and process\n",
    "    refpos = pd.read_csv(posfile, sep='\\t', comment='#',header=None)\n",
    "    \n",
    "    #0      22065657\n",
    "    #1      22065697\n",
    "    #2      22065904\n",
    "    #3      22065908\n",
    "    #4      22065974\n",
    "    #5      22065977\n",
    "    \n",
    "    refpos = pd.Series(refpos[1], index=range(len(refpos[1])))\n",
    "\n",
    "    #print(refpos[1])\n",
    "    \n",
    "    #infile is the input file: genotype data set to be imputed\n",
    "    df = pd.read_csv(infile, sep='\\t', comment='#',header=None)\n",
    "    \n",
    "    #0      22065657\n",
    "    #1      22066211\n",
    "    #2      22066363\n",
    "    #3      22066572\n",
    "    #4      22067004\n",
    "    #5      22067276\n",
    "    \n",
    "    inpos = pd.Series(range(len(df[1])), index=df[1])\n",
    "    \n",
    "    #print(inpos[2])\n",
    "    \n",
    "    #genetic variants are rows and samples are columns\n",
    "    #let's transpose so the variants become columns and samples are rows\n",
    "    df_T = df.transpose()\n",
    "    \n",
    "    new_df = 0\n",
    "    \n",
    "    if(categorical==\"False\"):\n",
    "        new_df = np.zeros((len(df_T)-9,len(refpos),2)) #subjects, variants, Allele counts\n",
    "    else:\n",
    "        new_df = np.zeros((len(df_T)-9,len(refpos)))  #subjects, variants\n",
    "    #print(new_df.shape)\n",
    "    i = 9 #RR column index\n",
    "    j = 0 #RR row index\n",
    "    idx = 0\n",
    "    print(\"Processing input data.\")\n",
    "    #print(categorical)\n",
    "    myidx = 0\n",
    "    \n",
    "    while i < len(df_T):\n",
    "        j = 0\n",
    "        while j < len(refpos): #\"|\" is present when phased data is proved, \"/\" is usually unphased\n",
    "            if(refpos[j] in inpos.keys()):\n",
    "                myidx = inpos[refpos[j]]\n",
    "                #print(j)\n",
    "                #print(inpos[refpos[j]])\n",
    "                #print(refpos[j])\n",
    "                #print(df[i][myidx])\n",
    "                #print(df[i+1][myidx])\n",
    "                if(df[i][myidx].startswith('1|1') or df[i][myidx].startswith('1/1')):\n",
    "                    if(categorical==\"True\"):\n",
    "                        new_df[idx][j] = 2\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([0,2])\n",
    "                        new_df[idx][j][0] = 0\n",
    "                        new_df[idx][j][1] = 2\n",
    "                elif(df[i][myidx].startswith('1|0') or df[i][myidx].startswith('0|1') or df[i][myidx].startswith('1/0') or df[i][myidx].startswith('0/1')):\n",
    "                    if(categorical==\"True\"):\n",
    "                        new_df[idx][j] = 1\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([1,1])\n",
    "                        new_df[idx][j][0] = 1\n",
    "                        new_df[idx][j][1] = 1\n",
    "                elif(df[i][myidx].startswith('0|0') or df[i][myidx].startswith('0/0')):\n",
    "                    if(categorical==\"True\"): \n",
    "                        new_df[idx][j] = 0\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([2,0])\n",
    "                        new_df[idx][j][0] = 2\n",
    "                        new_df[idx][j][1] = 0\n",
    "                else:\n",
    "                    if(categorical==\"True\"):\n",
    "                        new_df[idx][j] = -1\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([0,0]) \n",
    "                        new_df[idx][j][0] = 0 \n",
    "                        new_df[idx][j][1] = 0 \n",
    "            else:\n",
    "                if(categorical==\"True\"):\n",
    "                    new_df[idx][j] = -1\n",
    "                else:\n",
    "                    #new_df[idx][j] = np.array([0,0]) \n",
    "                    new_df[idx][j][0] = 0 \n",
    "                    new_df[idx][j][1] = 0 \n",
    "                #if(idx==0):\n",
    "                #    print(j)\n",
    "                #RR I forgot to mention that we have to take into account possible missing data\n",
    "                #RR in case there is missing data (NA, .|., -|-, or anything different from 0|0, 1|1, 0|1, 1|0) = 3\n",
    "            j += 1\n",
    "        i += 1\n",
    "        #pbar.update(1)\n",
    "        idx += 1\n",
    "\n",
    "    #print(\"processed_data\")\n",
    "    #for i in range(10):\n",
    "    #    print(new_df[i][0])\n",
    "\n",
    "    #the data needs to be flattened because the matrix multiplication step (x*W) \n",
    "    #doesn't support features with subfeatures (matrix of vectors)\n",
    "    #new_df = np.reshape(new_df, (new_df.shape[0],new_df.shape[1]*2))\n",
    "    #print(new_df.shape)\n",
    "    #pbar.close()\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time to load the data (sec): ', stop - start)\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(mydata, mask_rate=0.9, categorical=\"False\"):\n",
    "    start = timeit.default_timer()\n",
    "    # random matrix the same shape of your data\n",
    "    #print(len(mydata))\n",
    "    nmask = int(round(len(mydata[0])*mask_rate))\n",
    "    # random boolean mask for which values will be changed\n",
    "    maskindex = random.sample(range(0, len(mydata[0]-1)), nmask)\n",
    "    maskindex = [211, 654, 274, 410, 236, 731, 678, 490, 710, 650, 377, 826, 483, 828, 283, 237, 625, 844, 796, 428, 235, 238, 185, 790, 630, 89, 51, 760, 134, 421, 217, 138, 513, 116, 824, 623, 266, 142, 286, 158, 485, 309, 794, 546, 203, 809, 333, 593, 258, 172, 605, 293, 466, 693, 97, 234, 326, 764, 628, 372, 426, 46, 52, 752, 680, 493, 798, 507, 98, 243, 87, 31, 477, 510, 28, 264, 606, 166, 154, 63, 374, 547, 57, 91, 543, 818, 363, 122, 218, 219, 365, 624, 703, 821, 707, 191, 423, 61, 399, 738, 222, 540, 685, 801, 737, 104, 587, 793, 661, 486, 782, 37, 516, 772, 814, 305, 588, 103, 859, 762, 22, 567, 487, 223, 658, 541, 523, 33, 296, 465, 130, 626, 361, 761, 419, 111, 226, 622, 402, 163, 132, 691, 39, 648, 495, 287, 492, 40, 190, 501, 520, 827, 231, 770, 713, 659, 702, 340, 517, 24, 277, 749, 221, 524, 167, 504, 126, 531, 617, 527, 371, 742, 108, 719, 164, 392, 739, 602, 298, 41, 640, 674, 313, 646, 422, 566, 594, 290, 383, 187, 837, 112, 450, 603, 457, 805, 500, 148, 629, 225, 389, 21, 591, 627, 378, 3, 512, 449, 570, 140, 855, 263, 468, 143, 322, 544, 604, 451, 754, 242, 95, 10, 413, 49, 864, 347, 461, 141, 375, 689, 692, 706, 345, 542, 849, 145, 107, 803, 584, 780, 632, 385, 579, 503, 69, 415, 788, 81, 330, 643, 559, 56, 284, 438, 571, 718, 241, 34, 316, 532, 817, 775, 665, 58, 511, 787, 268, 536, 147, 379, 533, 637, 740, 14, 783, 118, 171, 168, 366, 84, 842, 853, 357, 319, 292, 88, 406, 324, 829, 765, 489, 408, 435, 444, 156, 215, 109, 644, 808, 607, 744, 663, 845, 4, 12, 220, 865, 356, 331, 418, 813, 315, 447, 454, 539, 779, 170, 251, 83, 631, 189, 73, 262, 105, 858, 615, 188, 353, 301, 581, 700, 350, 456, 730, 555, 664, 196, 70, 458, 529, 723, 725, 840, 32, 247, 60, 306, 439, 38, 573, 834, 822, 387, 48, 701, 216, 564, 810, 317, 442, 54, 327, 437, 254, 386, 240, 230, 734, 590, 151, 136, 472, 328, 836, 611, 515, 820, 460, 250, 589, 425, 5, 786, 497, 655, 666, 568, 657, 535, 561, 233, 835, 71, 125, 756, 131, 769, 161, 578, 862, 577, 653, 123, 720, 272, 181, 528, 552, 178, 807, 208, 152, 96, 434, 209, 785, 621, 619, 176, 462, 295, 610, 649, 745, 232, 514, 861, 667, 239, 229, 433, 260, 159, 113, 481, 360, 35, 436, 494, 397, 11, 416, 797, 72, 25, 213, 595, 320, 198, 348, 560, 395, 153, 771, 727, 318, 66, 139, 92, 99, 694, 411, 179, 502, 304, 804, 120, 252, 101, 750, 403, 9, 204, 480, 741, 811, 42, 592, 129, 598, 599, 124, 843, 329, 574, 224, 19, 420, 396, 310, 43, 370, 812, 792, 777, 521, 846, 635, 115, 393, 645, 609, 368, 616, 519, 160, 755, 380, 362, 711, 94, 311, 746, 409, 802, 776, 312, 68, 400, 816, 735, 668, 280, 373, 67, 30, 612, 660, 302, 509, 259, 173, 841, 390, 582, 79, 369, 200, 192, 26, 681, 733, 384, 526, 597, 585, 795, 246, 647, 554, 271, 833, 55, 257, 850, 478, 565, 417, 498, 269, 430, 613, 401, 128, 227, 537, 114, 0, 854, 248, 194, 699, 747, 407, 851, 228, 342, 86, 183, 155, 278, 508, 636, 614, 135, 459, 212, 424, 496, 452, 100, 273, 245, 684, 671, 766, 337, 484, 59, 376, 297, 106, 341, 1, 839, 344, 332, 146, 732, 714, 580, 180, 29, 355, 856, 276, 78, 767, 548, 214, 819, 18, 2, 62, 557, 76, 751, 275, 562, 499, 728, 525, 279, 15, 432, 45, 823, 119, 85, 475, 675, 569, 255, 300, 608, 690, 708, 784, 253, 6, 715, 736, 551, 729, 448, 717, 20, 102, 367, 679, 47, 575, 641, 455, 336, 832, 670, 323, 398, 556, 726, 64, 427, 202, 186, 530, 93, 848, 748, 695, 482, 36, 77, 82, 620, 388, 23, 474, 314, 470, 338, 669, 759, 860, 800, 696, 852, 743, 583, 618, 506, 601, 53, 65, 463, 633, 716, 169, 288, 414, 774, 291, 193, 596, 261, 768, 704, 453, 440, 359, 550, 381, 90, 473, 758, 303, 150, 294, 697, 799, 174, 197, 445, 265, 335, 639, 572, 446, 863, 825, 80, 299, 781, 133, 538, 144, 270, 206, 7, 207, 471, 175, 74, 753, 44, 712, 382, 722, 600, 334, 13, 182, 349, 476, 488, 165, 687, 709, 210, 351, 431, 576, 682, 321, 549, 308, 289, 50, 121]\n",
    "    #np.random.randint(0,len(mydata[0]),size=nmask)\n",
    "    print(\"Masking markers...\")\n",
    "    print(maskindex)\n",
    "    #mydata = np.transpose(mydata)\n",
    "    print(mydata.shape)\n",
    "    #mydata\n",
    "    \n",
    "    #pbar = tqdm(total = len(maskindex))\n",
    "    #for i in range(10):\n",
    "    #    print(mydata[i][0:11])\n",
    "\n",
    "\n",
    "    for i in maskindex:\n",
    "        #print(len(mydata[i]))\n",
    "        j = 0\n",
    "        while j < len(mydata):\n",
    "            if(categorical==\"True\"):\n",
    "                mydata[j][i]=-1\n",
    "            else:\n",
    "                mydata[j][i]=[0,0]\n",
    "            j=j+1\n",
    "        #pbar.update(1)\n",
    "        #print(mydata[i])\n",
    "    #mydata = np.transpose(mydata)\n",
    "    #print(mydata.shape)\n",
    "    #pbar.close()\n",
    "\n",
    "    #print(\"after masking:\")\n",
    "    #for i in range(10):\n",
    "    #    print(mydata[i][0:11])\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time to mask the data (sec): ', stop - start)  \n",
    "    return mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_maf_threshold(x, y, threshold1, threshold2, categorical=False):\n",
    "    \n",
    "    colsum=np.sum(y, axis=0)\n",
    "    indexes_to_keep = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0  \n",
    "    #print(len(MAF_all_var))\n",
    "    \n",
    "    if(threshold1!=0 or threshold2!=max(MAF_all_var)):\n",
    "          \n",
    "        #MAFs = calculate_MAF(y, categorical)\n",
    "    \n",
    "        while i < len(MAF_all_var):\n",
    "            if(MAF_all_var[i]>=threshold1 and MAF_all_var[i]<=threshold2):\n",
    "                if(categorical==True):\n",
    "                    if(colsum[j]!=0 or colsum[j+1]!=0 or colsum[j+2]!=0):\n",
    "                        indexes_to_keep.append(j)\n",
    "                        indexes_to_keep.append(j+1)\n",
    "                        indexes_to_keep.append(j+2)\n",
    "                elif(categorical==False):\n",
    "                    if(colsum[k]!=0 or colsum[k+1]!=0):\n",
    "                        indexes_to_keep.append(k)\n",
    "                        indexes_to_keep.append(k+1)            \n",
    "            i += 1\n",
    "            j += 3\n",
    "            k += 2\n",
    "        \n",
    "        #print(indexes_to_keep)\n",
    "        #print(len(x[0]))\n",
    "        #print(len(y[0]))\n",
    "        #print(len(filtered_data_x))\n",
    "        #print(len(filtered_data_y[0]))\n",
    "    else:\n",
    "\n",
    "        while i < len(MAF_all_var):\n",
    "            if(categorical==True):\n",
    "                if(colsum[j]!=0 or colsum[j+1]!=0 or colsum[j+2]!=0):\n",
    "                    indexes_to_keep.append(j)\n",
    "                    indexes_to_keep.append(j+1)\n",
    "                    indexes_to_keep.append(j+2)\n",
    "            elif(categorical==False):\n",
    "                if(colsum[k]!=0 or colsum[k+1]!=0):\n",
    "                    indexes_to_keep.append(k)\n",
    "                    indexes_to_keep.append(k+1)            \n",
    "            i += 1\n",
    "            j += 3\n",
    "            k += 2   \n",
    "            \n",
    "    getter = operator.itemgetter(indexes_to_keep)\n",
    "    filtered_data_x = list(map(list, map(getter, np.copy(x))))\n",
    "    filtered_data_y = list(map(list, map(getter, np.copy(y))))\n",
    "    \n",
    "    correct_prediction = np.equal( np.round( filtered_data_x ), np.round( filtered_data_y ) )\n",
    "    accuracy_per_marker = np.mean(correct_prediction.astype(float), 0)\n",
    "    accuracy = np.mean(accuracy_per_marker)\n",
    "\n",
    "    #correct_prediction = sess.run(tf.equal( tf.round( filtered_data_x ), tf.round( filtered_data_y ) ))\n",
    "    #accuracy_per_marker = sess.run(tf.reduce_mean(tf.cast(correct_prediction, tf.float32), 0))\n",
    "    #accuracy = sess.run(tf.reduce_mean(accuracy_per_marker))\n",
    "\n",
    "    return accuracy, accuracy_per_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAF_global_GPU(indexes, inx, categorical=False):\n",
    "    \n",
    "\n",
    "    j=0\n",
    "    if(do_parallel_MAF==True):\n",
    "        getter = operator.itemgetter(indexes)\n",
    "        x = list(map(list, map(getter, np.copy(inx))))\n",
    "    else:\n",
    "        x = inx\n",
    "    MAF_list = []\n",
    "        \n",
    "    #tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Session(config=config) as sess:        \n",
    "       \n",
    "        #print(\"LENGTH\", len(x[0]))\n",
    "        if(categorical==True):\n",
    "            while j < (len(x[0])):\n",
    "                ref = 0\n",
    "                alt = 0\n",
    "                MAF = 0        \n",
    "                for i in range(len(x)):\n",
    "                    if(i == 0):\n",
    "                        ref = sess.run(tf.add(ref,2))\n",
    "                    elif(i == 1):\n",
    "                        ref = sess.run(tf.add(ref,1))\n",
    "                        alt = sess.run(tf.add(alt,1))\n",
    "                    elif(i == 2):\n",
    "                        alt = sess.run(tf.add(alt,2))\n",
    "                if(alt<=ref):\n",
    "                    MAF=sess.run(tf.div(alt,tf.add(ref,alt)))\n",
    "                    #major=ref/len(y)\n",
    "                else:\n",
    "                    MAF=sess.run(tf.div(ref,tf.add(ref,alt)))\n",
    "                    #major=alt/len(y)\n",
    "                    #print(MAF)\n",
    "                MAF_list.append(MAF)\n",
    "                j+=1          \n",
    "        elif(categorical==False):\n",
    "            while j < (len(x[0])):\n",
    "                ref = 0\n",
    "                alt = 0\n",
    "                MAF = 0        \n",
    "                for i in range(len(x)):\n",
    "                    ref = sess.run(tf.add(ref,x[i][j][0]))\n",
    "                    alt = sess.run(tf.add(alt,x[i][j][1]))  \n",
    "                if(alt<=ref):\n",
    "                    MAF=sess.run(tf.div(alt,tf.add(ref,alt)))\n",
    "                    #major=ref/len(y)\n",
    "                else:\n",
    "                    MAF=sess.run(tf.div(ref,tf.add(ref,alt)))\n",
    "                MAF_list.append(MAF)    \n",
    "                j+=1\n",
    "    \n",
    "    #reset tensorflow session\n",
    "    #tf.reset_default_graph()\n",
    "    sess.close()\n",
    "    return MAF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAF_global(indexes, inx, categorical=False):\n",
    "    print(\"processing variants with indexes:\", indexes)\n",
    "    j=0\n",
    "    if(do_parallel_MAF==True):\n",
    "        getter = operator.itemgetter(indexes)\n",
    "        x = list(map(list, map(getter, np.copy(inx))))\n",
    "    else:\n",
    "        x = inx\n",
    "    MAF_list = []\n",
    "    #print(\"LENGTH\", len(x[0]))\n",
    "    if(categorical==True):\n",
    "        while j < (len(x[0])):\n",
    "            ref = 0\n",
    "            alt = 0\n",
    "            MAF = 0        \n",
    "            for i in range(len(x)):\n",
    "                if(i == 0):\n",
    "                    ref+=2\n",
    "                elif(i == 1):\n",
    "                    ref+=1\n",
    "                    alt+=1\n",
    "                elif(i == 2):\n",
    "                    alt+=2\n",
    "            if(alt<=ref):\n",
    "                MAF=alt/(ref+alt)\n",
    "                #major=ref/len(y)\n",
    "            else:\n",
    "                MAF=ref/(ref+alt)\n",
    "                #major=alt/len(y)\n",
    "                #print(MAF)\n",
    "            MAF_list.append(MAF)    \n",
    "            j+=1          \n",
    "    elif(categorical==False):\n",
    "        while j < (len(x[0])):\n",
    "            ref = 0\n",
    "            alt = 0\n",
    "            MAF = 0        \n",
    "            for i in range(len(x)):\n",
    "                ref+=x[i][j][0]\n",
    "                alt+=x[i][j][1]   \n",
    "            if(alt<=ref):\n",
    "                MAF=alt/(ref+alt)\n",
    "                #major=ref/len(y)\n",
    "            else:\n",
    "                MAF=ref/(ref+alt)\n",
    "            MAF_list.append(MAF)    \n",
    "            j+=1\n",
    "    \n",
    "    print(\"processing variants done for indexes:\", indexes)\n",
    "\n",
    "    return MAF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#split inut data into chunks so we can prepare batches in parallel\n",
    "def chunk(L,nchunks):\n",
    "    L2 = list()\n",
    "    j = round(len(L)/nchunks)\n",
    "    chunk_size = j\n",
    "    i = 0\n",
    "    while i < len(L):\n",
    "        chunk = L[i:j]\n",
    "        L2.append(chunk)\n",
    "        i = j\n",
    "        j += chunk_size\n",
    "        if(j>len(L)):\n",
    "            j = len(L)\n",
    "    return L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mydata):\n",
    "    #subjects, SNP, REF/ALT counts\n",
    "    if(len(mydata.shape) == 3):\n",
    "        mydata = np.reshape(mydata, (mydata.shape[0],-1))\n",
    "    else:#do one hot encoding, depth=3 because missing (-1) is encoded to all zeroes\n",
    "        mydata = tf.one_hot(indices=mydata, depth=3)\n",
    "        mydata = tf.layers.flatten(mydata)#flattening to simplify calculations later (matmul, add, etc)\n",
    "    return mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input data.\n",
      "Time to load the data (sec):  27.102002907002316\n"
     ]
    }
   ],
   "source": [
    "new_df = process_data(\"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.pos.clean4\", \"ARIC_PLINK_flagged_chromosomal_abnormalities_zeroed_out_bed.lifted_NCBI36_to_GRCh37.GH.ancestry-1.chr9_intersect1.vcf.gz.9p21.3.recode.vcf\", categorical=\"False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input data.\n",
      "Time to load the data (sec):  181.9635181800004\n"
     ]
    }
   ],
   "source": [
    "new_df_obs = process_data(\"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.pos.clean4\", \"c1_ARIC_WGS_Freeze3.lifted_already_GRCh37_intersect1.vcf.gz.9p21.3.recode.vcf\", categorical=\"False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_new_df_obs = np.copy(new_df_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = mask_data(np.copy(orig_new_df_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df_obs2 = process_data(\"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.pos.clean3\", \"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.clean3\", categorical=\"False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = flatten(new_df.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 1692)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_obs = flatten(new_df_obs.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 1692)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_df_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df_obs2 = flatten(new_df_obs2.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(new_df_obs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/rdias/myscripts/raqueld/Autoencoder_tensorflow/test_data_augmentation_100b/inference_model-1.ckpt\n",
      "<tf.Variable 'weights/w_encoder_h1:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'weights/w_decoder_h1:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'biases/b_encoder_b1:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'biases/b_decoder_b1:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'dense/kernel:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'dense/bias:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'weights/w_encoder_h1/optimizer:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'weights/w_encoder_h1/optimizer_1:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'weights/w_decoder_h1/optimizer:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'weights/w_decoder_h1/optimizer_1:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'biases/b_encoder_b1/optimizer:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'biases/b_encoder_b1/optimizer_1:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'biases/b_decoder_b1/optimizer:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'biases/b_decoder_b1/optimizer_1:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'dense/kernel/optimizer:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'dense/kernel/optimizer_1:0' shape=(1692, 1692) dtype=float32_ref>\n",
      "<tf.Variable 'dense/bias/optimizer:0' shape=(1692,) dtype=float32_ref>\n",
      "<tf.Variable 'dense/bias/optimizer_1:0' shape=(1692,) dtype=float32_ref>\n",
      "[[1. 1. 2. ... 0. 1. 1.]\n",
      " [1. 1. 2. ... 0. 2. 0.]\n",
      " [0. 2. 2. ... 0. 1. 1.]\n",
      " ...\n",
      " [1. 1. 2. ... 0. 1. 1.]\n",
      " [2. 0. 2. ... 0. 0. 2.]\n",
      " [1. 1. 2. ... 0. 1. 1.]]\n",
      "\n",
      "****\n",
      "\n",
      "[[0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 2. 0.]\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 2.]\n",
      " [0. 0. 0. ... 0. 1. 1.]]\n",
      "\n",
      "****\n",
      "\n",
      "\n",
      "****\n",
      "\n",
      "[[0.       2.016341 2.016341 ... 0.       0.       2.016341]\n",
      " [0.       2.016341 2.016341 ... 0.       0.       2.016341]\n",
      " [0.       2.016341 2.016341 ... 0.       0.       2.016341]\n",
      " ...\n",
      " [0.       2.016341 2.016341 ... 0.       0.       2.016341]\n",
      " [0.       2.016341 2.016341 ... 0.       0.       2.016341]\n",
      " [0.       2.016341 2.016341 ... 0.       0.       2.016341]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define layer size\n",
    "#n_input = len(new_df[0])     # input features N_variants\n",
    "#n_hidden_1 = n_input  # hidden layer for encoder, equal to input number of features for now\n",
    "#print(n_input)\n",
    "#tf input\n",
    "#X = tf.placeholder(\"float\", [None, n_input])\n",
    "#Y = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "    \n",
    "#biases = {\n",
    "#    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#    'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n",
    "#}\n",
    "\n",
    "    #print(X.get_shape())\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=False)\n",
    "config.intra_op_parallelism_threads = 4\n",
    "config.inter_op_parallelism_threads = 4\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.15\n",
    "#config.gpu_options.allow_growth=True\n",
    "#with tf.device('/device:GPU:0'):  # Replace with device you are interested in\n",
    "bytes_in_use = BytesInUse()\n",
    "    \n",
    "sess=tf.Session(config=config) \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    #saver.restore(sess,tf.train.latest_checkpoint('/home/rdias/myscripts/raqueld/Autoencoder_tensorflow/10-fold_CV_F_new_backup2/'))\n",
    "    saver = tf.train.import_meta_graph('/home/rdias/myscripts/raqueld/Autoencoder_tensorflow/test_data_augmentation_100b/inference_model-1.ckpt.meta')\n",
    "\n",
    "    saver.restore(sess,'/home/rdias/myscripts/raqueld/Autoencoder_tensorflow/test_data_augmentation_100b/inference_model-1.ckpt')\n",
    "    #with tf.device('/device:GPU:0'): \n",
    "    # Access saved Variables directly\n",
    "    graph = sess.graph\n",
    "    for g_var in tf.global_variables():\n",
    "        print(g_var)\n",
    "    for g_var in tf.local_variables():\n",
    "        print(g_var)\n",
    "\n",
    "    #optimizer = graph.get_operation_by_name( \"optimizer\" )\n",
    "    print(sess.run('Y:0', feed_dict={\"X:0\": new_df, \"Y:0\": new_df_obs}))\n",
    "    print(\"\\n****\\n\")\n",
    "    #print(new_df_obs)\n",
    "    #print(\"\\n****\\n\")\n",
    "    print(sess.run('X:0', feed_dict={\"X:0\": new_df, \"Y:0\": new_df_obs}))\n",
    "    print(\"\\n****\\n\")\n",
    "    #print(new_df)\n",
    "    y_pred = (sess.run('y_pred:0', feed_dict={\"X:0\": new_df, \"Y:0\": new_df_obs}))\n",
    "    print(\"\\n****\\n\")\n",
    "    print(y_pred)\n",
    "    \n",
    "    j=0\n",
    "    '''    while j < (len(y_pred[0])-1):\n",
    "        test1 = np.zeros(shape=(len(y_pred),2))\n",
    "        test2 = np.zeros(shape=(len(y_pred),2))\n",
    "        for i in range(len(y_pred)):\n",
    "            test1[i][0] = y_pred[i][j]\n",
    "            test2[i][0] =  new_df_obs[i][j]\n",
    "            j +=1\n",
    "            test1[i][1] = y_pred[i][j]\n",
    "            test2[i][1] =  new_df_obs[i][j]       \n",
    "        \n",
    "        correct_prediction = sess.run( tf.equal( tf.round( test1 ), tf.round( test2 ) ) )\n",
    "        accuracy = sess.run(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n",
    "        print(accuracy)\n",
    "        j += 1\n",
    "        \n",
    "       '''\n",
    " \n",
    "    correct_prediction = sess.run( tf.equal( tf.round( tf.cast(y_pred, tf.float64) ), tf.round( new_df_obs ) ) )\n",
    "\n",
    "            \n",
    "    correct_prediction = sess.run( tf.equal( tf.round( tf.cast(y_pred, tf.float64) ), tf.round( new_df_obs ) ) )\n",
    "    accuracy = sess.run(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7947135\n"
     ]
    }
   ],
   "source": [
    "#chkp.print_tensors_in_checkpoint_file(\"/tmp/model.ckpt\", tensor_name='v2', all_tensors=False)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "processing variants with indexes: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]\n",
      "processing variants done for indexes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n",
      "processing variants done for indexes: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n",
      "processing variants done for indexes: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]\n",
      "processing variants done for indexes: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]\n",
      "processing variants done for indexes: [96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants done for indexes: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143]\n",
      "processing variants with indexes: [192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215]\n",
      "processing variants done for indexes: [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants done for indexes: [168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191]\n",
      "processing variants done for indexes: [192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215]\n",
      "processing variants with indexes: [240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]\n",
      "processing variants done for indexes: [216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants done for indexes: [240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263]\n",
      "processing variants with indexes: [312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335]\n",
      "processing variants done for indexes: [264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359]\n",
      "processing variants done for indexes: [288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants done for indexes: [312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335]\n",
      "processing variants with indexes: [384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407]\n",
      "processing variants done for indexes: [336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants done for indexes: [360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383]\n",
      "processing variants with indexes: [432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455]\n",
      "processing variants done for indexes: [384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479]\n",
      "processing variants done for indexes: [408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants done for indexes: [432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455]\n",
      "processing variants with indexes: [504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527]\n",
      "processing variants done for indexes: [456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants done for indexes: [480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503]\n",
      "processing variants done for indexes: [504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527]\n",
      "processing variants with indexes: [552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599]\n",
      "processing variants done for indexes: [528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551]\n",
      "processing variants with indexes: [600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing variants with indexes: [624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647]\n",
      "processing variants done for indexes: [576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599]\n",
      "processing variants done for indexes: [552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575]\n",
      "processing variants with indexes: [648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671]\n",
      "processing variants done for indexes: [600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623]\n",
      "processing variants with indexes: [672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695]\n",
      "processing variants done for indexes: [624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647]\n",
      "processing variants with indexes: [696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719]\n",
      "processing variants with indexes: [720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743]\n",
      "processing variants done for indexes: [672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695]\n",
      "processing variants done for indexes: [648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671]\n",
      "processing variants with indexes: [744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767]\n",
      "processing variants done for indexes: [696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719]\n",
      "processing variants with indexes: [768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791]\n",
      "processing variants done for indexes: [720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743]\n",
      "processing variants with indexes: [792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815]\n",
      "processing variants done for indexes: [744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767]\n",
      "processing variants with indexes: [816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839]\n",
      "processing variants done for indexes: [768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791]\n",
      "processing variants with indexes: [840, 841, 842, 843, 844, 845]\n",
      "processing variants done for indexes: [840, 841, 842, 843, 844, 845]\n",
      "processing variants done for indexes: [792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815]\n",
      "processing variants done for indexes: [816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839]\n",
      "merging results...\n"
     ]
    }
   ],
   "source": [
    "global MAF_all_var\n",
    "\n",
    "indexes=list(range(len(orig_new_df_obs[0])))\n",
    "\n",
    "if(do_parallel_MAF == False):        \n",
    "        \n",
    "    MAF_all_var = calculate_MAF_global_GPU(indexes, results, categorical)\n",
    "    \n",
    "else:\n",
    "    chunks = chunk(indexes, int(round(len(indexes)/ncores)) )\n",
    "        \n",
    "    pool = multiprocessing.Pool(ncores)\n",
    "\n",
    "    MAF_all_var = pool.map(partial(calculate_MAF_global, inx=orig_new_df_obs, categorical=False),chunks)\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    pool.join()\n",
    "    print(\"merging results...\")\n",
    "        #merge outputs from all processes, reshaping nested list\n",
    "    MAF_all_var = [item for sublist in MAF_all_var for item in sublist]\n",
    "\n",
    "#print(indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2, accuracy_per_marker = accuracy_maf_threshold(y_pred, new_df_obs, 0, 1, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876449938949939\n"
     ]
    }
   ],
   "source": [
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAF: 0.39629120879120877 Accuracy: 0.1614010989010989\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.1614010989010989\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.0037774725274725275 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.07177197802197802 Accuracy: 0.9993131868131868\n",
      "MAF: 0.39629120879120877 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9979395604395604\n",
      "MAF: 0.014766483516483516 Accuracy: 0.9979395604395604\n",
      "MAF: 0.395260989010989 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9986263736263736\n",
      "MAF: 0.002403846153846154 Accuracy: 0.992445054945055\n",
      "MAF: 0.39732142857142855 Accuracy: 0.992445054945055\n",
      "MAF: 0.39732142857142855 Accuracy: 0.9986263736263736\n",
      "MAF: 0.4223901098901099 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0027472527472527475 Accuracy: 0.9993131868131868\n",
      "MAF: 0.39629120879120877 Accuracy: 0.0\n",
      "MAF: 0.02197802197802198 Accuracy: 0.8626373626373627\n",
      "MAF: 0.007554945054945055 Accuracy: 0.8626373626373627\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0020604395604395605 Accuracy: 1.0\n",
      "MAF: 0.395260989010989 Accuracy: 1.0\n",
      "MAF: 0.007211538461538462 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.0\n",
      "MAF: 0.006524725274725275 Accuracy: 1.0\n",
      "MAF: 0.003434065934065934 Accuracy: 0.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.07211538461538461 Accuracy: 0.1614010989010989\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.36881868131868134\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9986263736263736\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.004120879120879121 Accuracy: 1.0\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9711538461538461\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9711538461538461\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.1614010989010989\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.1614010989010989\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.39938186813186816 Accuracy: 1.0\n",
      "MAF: 0.007211538461538462 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0037774725274725275 Accuracy: 0.9951923076923077\n",
      "MAF: 0.003434065934065934 Accuracy: 0.9951923076923077\n",
      "MAF: 0.002403846153846154 Accuracy: 1.0\n",
      "MAF: 0.007898351648351648 Accuracy: 1.0\n",
      "MAF: 0.0030906593406593405 Accuracy: 0.17307692307692307\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.17307692307692307\n",
      "MAF: 0.4948489010989011 Accuracy: 0.17307692307692307\n",
      "MAF: 0.4955357142857143 Accuracy: 0.17307692307692307\n",
      "MAF: 0.004120879120879121 Accuracy: 0.1813186813186813\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.33653846153846156\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.0\n",
      "MAF: 0.023695054945054944 Accuracy: 1.0\n",
      "MAF: 0.49244505494505497 Accuracy: 0.9945054945054945\n",
      "MAF: 0.4227335164835165 Accuracy: 0.9945054945054945\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.1620879120879121\n",
      "MAF: 0.0027472527472527475 Accuracy: 0.1620879120879121\n",
      "MAF: 0.003434065934065934 Accuracy: 0.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.007554945054945055 Accuracy: 0.9567307692307693\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9567307692307693\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9848901098901099\n",
      "MAF: 0.07589285714285714 Accuracy: 0.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0027472527472527475 Accuracy: 1.0\n",
      "MAF: 0.009271978021978022 Accuracy: 0.0\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9958791208791209\n",
      "MAF: 0.0037774725274725275 Accuracy: 0.9958791208791209\n",
      "MAF: 0.009271978021978022 Accuracy: 0.1614010989010989\n",
      "MAF: 0.010302197802197802 Accuracy: 0.1614010989010989\n",
      "MAF: 0.004464285714285714 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.479739010989011 Accuracy: 1.0\n",
      "MAF: 0.002403846153846154 Accuracy: 1.0\n",
      "MAF: 0.009271978021978022 Accuracy: 0.9855769230769231\n",
      "MAF: 0.009271978021978022 Accuracy: 0.9855769230769231\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9979395604395604\n",
      "MAF: 0.49107142857142855 Accuracy: 0.9979395604395604\n",
      "MAF: 0.009271978021978022 Accuracy: 0.9869505494505495\n",
      "MAF: 0.4921016483516483 Accuracy: 0.9869505494505495\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.9931318681318682\n",
      "MAF: 0.009271978021978022 Accuracy: 0.9931318681318682\n",
      "MAF: 0.3021978021978022 Accuracy: 0.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.8626373626373627\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.8626373626373627\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.002403846153846154 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.009271978021978022 Accuracy: 1.0\n",
      "MAF: 0.009271978021978022 Accuracy: 0.9972527472527473\n",
      "MAF: 0.010302197802197802 Accuracy: 0.9972527472527473\n",
      "MAF: 0.007211538461538462 Accuracy: 1.0\n",
      "MAF: 0.001717032967032967 Accuracy: 1.0\n",
      "MAF: 0.008928571428571428 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9986263736263736\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9965659340659341\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9965659340659341\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.4996565934065934 Accuracy: 0.9986263736263736\n",
      "MAF: 0.009615384615384616 Accuracy: 1.0\n",
      "MAF: 0.007211538461538462 Accuracy: 1.0\n",
      "MAF: 0.19951923076923078 Accuracy: 1.0\n",
      "MAF: 0.4993131868131868 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.008585164835164836 Accuracy: 1.0\n",
      "MAF: 0.09993131868131869 Accuracy: 0.9917582417582418\n",
      "MAF: 0.09443681318681318 Accuracy: 0.9917582417582418\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9972527472527473\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9972527472527473\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.0\n",
      "MAF: 0.0020604395604395605 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9972527472527473\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9972527472527473\n",
      "MAF: 0.024725274725274724 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.9986263736263736\n",
      "MAF: 0.03777472527472527 Accuracy: 0.9986263736263736\n",
      "MAF: 0.41346153846153844 Accuracy: 0.9986263736263736\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9986263736263736\n",
      "MAF: 0.059065934065934064 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.49828296703296704 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0027472527472527475 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.3784340659340659\n",
      "MAF: 0.059065934065934064 Accuracy: 0.1771978021978022\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.9855769230769231\n",
      "MAF: 0.005837912087912088 Accuracy: 0.9855769230769231\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.03021978021978022 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0020604395604395605 Accuracy: 1.0\n",
      "MAF: 0.49038461538461536 Accuracy: 1.0\n",
      "MAF: 0.002403846153846154 Accuracy: 0.992445054945055\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.992445054945055\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9931318681318682\n",
      "MAF: 0.17994505494505494 Accuracy: 0.9931318681318682\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9951923076923077\n",
      "MAF: 0.0027472527472527475 Accuracy: 0.9951923076923077\n",
      "MAF: 0.007554945054945055 Accuracy: 1.0\n",
      "MAF: 0.28708791208791207 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9842032967032966\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9842032967032966\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9938186813186813\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9938186813186813\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9993131868131868\n",
      "MAF: 0.49690934065934067 Accuracy: 0.9993131868131868\n",
      "MAF: 0.49690934065934067 Accuracy: 0.25618131868131866\n",
      "MAF: 0.4996565934065934 Accuracy: 0.2664835164835165\n",
      "MAF: 0.004120879120879121 Accuracy: 0.2657967032967033\n",
      "MAF: 0.001717032967032967 Accuracy: 0.2657967032967033\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.018543956043956044 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.011675824175824176 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.004464285714285714 Accuracy: 1.0\n",
      "MAF: 0.002403846153846154 Accuracy: 0.992445054945055\n",
      "MAF: 0.002403846153846154 Accuracy: 0.0006868131868131869\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.002403846153846154 Accuracy: 1.0\n",
      "MAF: 0.0013736263736263737 Accuracy: 1.0\n",
      "MAF: 0.01304945054945055 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9526098901098901\n",
      "MAF: 0.009615384615384616 Accuracy: 0.9526098901098901\n",
      "MAF: 0.001717032967032967 Accuracy: 0.24107142857142858\n",
      "MAF: 0.01098901098901099 Accuracy: 0.25618131868131866\n",
      "MAF: 0.0030906593406593405 Accuracy: 0.18681318681318682\n",
      "MAF: 0.002403846153846154 Accuracy: 0.34134615384615385\n",
      "MAF: 0.002403846153846154 Accuracy: 0.9993131868131868\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9993131868131868\n",
      "MAF: 0.004807692307692308 Accuracy: 0.9945054945054945\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9945054945054945\n",
      "MAF: 0.006868131868131868 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.4958791208791209 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.9931318681318682\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9931318681318682\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9993131868131868\n",
      "MAF: 0.010645604395604396 Accuracy: 0.9848901098901099\n",
      "MAF: 0.002403846153846154 Accuracy: 0.9848901098901099\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.001717032967032967 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.008241758241758242 Accuracy: 0.9993131868131868\n",
      "MAF: 0.008928571428571428 Accuracy: 0.9986263736263736\n",
      "MAF: 0.4955357142857143 Accuracy: 0.9986263736263736\n",
      "MAF: 0.4948489010989011 Accuracy: 1.0\n",
      "MAF: 0.0027472527472527475 Accuracy: 1.0\n",
      "MAF: 0.0020604395604395605 Accuracy: 0.006181318681318681\n",
      "MAF: 0.0027472527472527475 Accuracy: 0.006181318681318681\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.47836538461538464 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0030906593406593405 Accuracy: 1.0\n",
      "MAF: 0.002403846153846154 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9945054945054945\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9945054945054945\n",
      "MAF: 0.48214285714285715 Accuracy: 1.0\n",
      "MAF: 0.0020604395604395605 Accuracy: 1.0\n",
      "MAF: 0.002403846153846154 Accuracy: 1.0\n",
      "MAF: 0.004120879120879121 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 1.0\n",
      "MAF: 0.001717032967032967 Accuracy: 0.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9814560439560439\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.9972527472527473\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9972527472527473\n",
      "MAF: 0.3853021978021978 Accuracy: 0.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.992445054945055\n",
      "MAF: 0.05013736263736264 Accuracy: 0.992445054945055\n",
      "MAF: 0.49347527472527475 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.11469780219780219 Accuracy: 1.0\n",
      "MAF: 0.007898351648351648 Accuracy: 1.0\n",
      "MAF: 0.4168956043956044 Accuracy: 0.9814560439560439\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9814560439560439\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9793956043956044\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.0\n",
      "MAF: 0.48145604395604397 Accuracy: 0.9910714285714286\n",
      "MAF: 0.0020604395604395605 Accuracy: 0.9910714285714286\n",
      "MAF: 0.001717032967032967 Accuracy: 0.0\n",
      "MAF: 0.0013736263736263737 Accuracy: 1.0\n",
      "MAF: 0.4800824175824176 Accuracy: 0.9986263736263736\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.001717032967032967 Accuracy: 0.23351648351648352\n",
      "MAF: 0.001717032967032967 Accuracy: 0.23351648351648352\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9951923076923077\n",
      "MAF: 0.0020604395604395605 Accuracy: 0.9951923076923077\n",
      "MAF: 0.005494505494505495 Accuracy: 1.0\n",
      "MAF: 0.0020604395604395605 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9814560439560439\n",
      "MAF: 0.0030906593406593405 Accuracy: 0.9814560439560439\n",
      "MAF: 0.0013736263736263737 Accuracy: 1.0\n",
      "MAF: 0.0030906593406593405 Accuracy: 1.0\n",
      "MAF: 0.4642857142857143 Accuracy: 1.0\n",
      "MAF: 0.004464285714285714 Accuracy: 1.0\n",
      "MAF: 0.006524725274725275 Accuracy: 0.9814560439560439\n",
      "MAF: 0.4838598901098901 Accuracy: 0.9814560439560439\n",
      "MAF: 0.004807692307692308 Accuracy: 1.0\n",
      "MAF: 0.4649725274725275 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9993131868131868\n",
      "MAF: 0.004120879120879121 Accuracy: 0.9993131868131868\n",
      "MAF: 0.026785714285714284 Accuracy: 0.0\n",
      "MAF: 0.4632554945054945 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.2438186813186813\n",
      "MAF: 0.11641483516483517 Accuracy: 0.2438186813186813\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9814560439560439\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9814560439560439\n",
      "MAF: 0.010302197802197802 Accuracy: 0.2651098901098901\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.2616758241758242\n",
      "MAF: 0.010645604395604396 Accuracy: 0.9979395604395604\n",
      "MAF: 0.003434065934065934 Accuracy: 0.9979395604395604\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9814560439560439\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9814560439560439\n",
      "MAF: 0.010645604395604396 Accuracy: 0.10164835164835165\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.10164835164835165\n",
      "MAF: 0.004120879120879121 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.01098901098901099 Accuracy: 0.9993131868131868\n",
      "MAF: 0.46565934065934067 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.009958791208791208 Accuracy: 0.9986263736263736\n",
      "MAF: 0.004807692307692308 Accuracy: 0.9986263736263736\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.0\n",
      "MAF: 0.4632554945054945 Accuracy: 0.9951923076923077\n",
      "MAF: 0.003434065934065934 Accuracy: 0.9951923076923077\n",
      "MAF: 0.003434065934065934 Accuracy: 0.9993131868131868\n",
      "MAF: 0.46188186813186816 Accuracy: 0.9993131868131868\n",
      "MAF: 0.006181318681318681 Accuracy: 0.9814560439560439\n",
      "MAF: 0.4739010989010989 Accuracy: 0.9814560439560439\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9814560439560439\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9814560439560439\n",
      "MAF: 0.015796703296703296 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9793956043956044\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9793956043956044\n",
      "MAF: 0.010645604395604396 Accuracy: 0.9855769230769231\n",
      "MAF: 0.01098901098901099 Accuracy: 0.9855769230769231\n",
      "MAF: 0.0013736263736263737 Accuracy: 1.0\n",
      "MAF: 0.46050824175824173 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9965659340659341\n",
      "MAF: 0.008585164835164836 Accuracy: 0.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.4680631868131868 Accuracy: 1.0\n",
      "MAF: 0.004464285714285714 Accuracy: 0.0\n",
      "MAF: 0.004464285714285714 Accuracy: 1.0\n",
      "MAF: 0.001717032967032967 Accuracy: 0.0\n",
      "MAF: 0.004464285714285714 Accuracy: 1.0\n",
      "MAF: 0.4680631868131868 Accuracy: 1.0\n",
      "MAF: 0.0027472527472527475 Accuracy: 1.0\n",
      "MAF: 0.4642857142857143 Accuracy: 0.9821428571428571\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9821428571428571\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.4635989010989011 Accuracy: 0.9986263736263736\n",
      "MAF: 0.4543269230769231 Accuracy: 1.0\n",
      "MAF: 0.4646291208791209 Accuracy: 1.0\n",
      "MAF: 0.4694368131868132 Accuracy: 0.9965659340659341\n",
      "MAF: 0.005151098901098901 Accuracy: 0.9965659340659341\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9993131868131868\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9993131868131868\n",
      "MAF: 0.002403846153846154 Accuracy: 0.9972527472527473\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.9972527472527473\n",
      "MAF: 0.008241758241758242 Accuracy: 0.9986263736263736\n",
      "MAF: 0.005494505494505495 Accuracy: 0.9986263736263736\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.2554945054945055\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.2554945054945055\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0010302197802197802 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9807692307692307\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9807692307692307\n",
      "MAF: 0.47149725274725274 Accuracy: 0.9855769230769231\n",
      "MAF: 0.0037774725274725275 Accuracy: 0.9855769230769231\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.001717032967032967 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.007211538461538462 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.6394230769230769\n",
      "MAF: 0.4536401098901099 Accuracy: 0.6394230769230769\n",
      "MAF: 0.0027472527472527475 Accuracy: 0.25274725274725274\n",
      "MAF: 0.07074175824175824 Accuracy: 0.25137362637362637\n",
      "MAF: 0.0020604395604395605 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0020604395604395605 Accuracy: 0.9993131868131868\n",
      "MAF: 0.0037774725274725275 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9828296703296703\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9828296703296703\n",
      "MAF: 0.01304945054945055 Accuracy: 0.8138736263736264\n",
      "MAF: 0.0030906593406593405 Accuracy: 0.8138736263736264\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.8255494505494505\n",
      "MAF: 0.0006868131868131869 Accuracy: 0.8255494505494505\n",
      "MAF: 0.001717032967032967 Accuracy: 0.9972527472527473\n",
      "MAF: 0.49004120879120877 Accuracy: 0.9972527472527473\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.0013736263736263737 Accuracy: 1.0\n",
      "MAF: 0.0013736263736263737 Accuracy: 0.9993131868131868\n",
      "MAF: 0.4807692307692308 Accuracy: 0.9993131868131868\n",
      "MAF: 0.46600274725274726 Accuracy: 0.9993131868131868\n",
      "MAF: 0.49759615384615385 Accuracy: 0.9993131868131868\n",
      "MAF: 0.49862637362637363 Accuracy: 1.0\n",
      "MAF: 0.49759615384615385 Accuracy: 1.0\n",
      "MAF: 0.4879807692307692 Accuracy: 0.9958791208791209\n",
      "MAF: 0.0010302197802197802 Accuracy: 0.9958791208791209\n",
      "MAF: 0.4955357142857143 Accuracy: 0.9993131868131868\n",
      "MAF: 0.4886675824175824 Accuracy: 0.9993131868131868\n",
      "MAF: 0.010645604395604396 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 1.0\n",
      "MAF: 0.00034340659340659343 Accuracy: 0.9986263736263736\n",
      "MAF: 0.48695054945054944 Accuracy: 0.9986263736263736\n",
      "MAF: 0.008241758241758242 Accuracy: 0.9505494505494505\n",
      "MAF: 0.4962225274725275 Accuracy: 0.9505494505494505\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in range(len(MAF_all_var)):\n",
    "    if(MAF_all_var[i]>0):\n",
    "        print(\"MAF:\",MAF_all_var[i],\"Accuracy:\",accuracy_per_marker[j])\n",
    "        j=j+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2, accuracy_per_marker = accuracy_maf_threshold(y_pred, new_df_obs, 0, 0.005, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9545521185545224\n"
     ]
    }
   ],
   "source": [
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2, accuracy_per_marker = accuracy_maf_threshold(y_pred, new_df_obs, 0.005, 1, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5800037097938558\n"
     ]
    }
   ],
   "source": [
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2, accuracy_per_marker = accuracy_maf_threshold(y_pred, new_df_obs, 0, 0.1, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948202838827839\n"
     ]
    }
   ],
   "source": [
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2, accuracy_per_marker = accuracy_maf_threshold(y_pred, new_df_obs, 0.1, 1, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2649904443382704\n"
     ]
    }
   ],
   "source": [
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
