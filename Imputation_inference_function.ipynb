{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.memory_stats.python.ops.memory_stats_ops import BytesInUse\n",
    "\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# sorting results\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import timeit #measure runtime\n",
    "import random #masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(posfile, infile, categorical=\"False\"):\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    #Header and column names start with hastag, skip those\n",
    "    #posfile should contain 2 columns separated by tab: 1st = chromosome ID, 2nd = position\n",
    "    #vcf can be imported as posfile as well, but will take much longer to read and process\n",
    "    refpos = pd.read_csv(posfile, sep='\\t', comment='#',header=None)\n",
    "    \n",
    "    #0      22065657\n",
    "    #1      22065697\n",
    "    #2      22065904\n",
    "    #3      22065908\n",
    "    #4      22065974\n",
    "    #5      22065977\n",
    "    \n",
    "    refpos = pd.Series(refpos[1], index=range(len(refpos[1])))\n",
    "\n",
    "    #print(refpos[1])\n",
    "    \n",
    "    #infile is the input file: genotype data set to be imputed\n",
    "    df = pd.read_csv(infile, sep='\\t', comment='#',header=None)\n",
    "    \n",
    "    #0      22065657\n",
    "    #1      22066211\n",
    "    #2      22066363\n",
    "    #3      22066572\n",
    "    #4      22067004\n",
    "    #5      22067276\n",
    "    \n",
    "    inpos = pd.Series(range(len(df[1])), index=df[1])\n",
    "    \n",
    "    #print(inpos[2])\n",
    "    \n",
    "    #genetic variants are rows and samples are columns\n",
    "    #let's transpose so the variants become columns and samples are rows\n",
    "    df_T = df.transpose()\n",
    "    \n",
    "    new_df = 0\n",
    "    \n",
    "    if(categorical==\"False\"):\n",
    "        new_df = np.zeros((len(df_T)-9,len(refpos),2)) #subjects, variants, Allele counts\n",
    "    else:\n",
    "        new_df = np.zeros((len(df_T)-9,len(refpos)))  #subjects, variants\n",
    "    #print(new_df.shape)\n",
    "    i = 9 #RR column index\n",
    "    j = 0 #RR row index\n",
    "    idx = 0\n",
    "    print(\"Processing input data.\")\n",
    "    print(categorical)\n",
    "    myidx = 0\n",
    "    \n",
    "    while i < len(df_T):\n",
    "        j = 0\n",
    "        while j < len(refpos): #\"|\" is present when phased data is proved, \"/\" is usually unphased\n",
    "            if(refpos[j] in inpos.keys()):\n",
    "                myidx = inpos[refpos[j]]\n",
    "                #print(j)\n",
    "                #print(inpos[refpos[j]])\n",
    "                #print(refpos[j])\n",
    "                #print(df[i][myidx])\n",
    "                #print(df[i+1][myidx])\n",
    "                if(df[i][myidx].startswith('1|1') or df[i][myidx].startswith('1/1')):\n",
    "                    if(categorical==\"True\"):\n",
    "                        new_df[idx][j] = 2\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([0,2])\n",
    "                        new_df[idx][j][0] = 0\n",
    "                        new_df[idx][j][1] = 2\n",
    "                elif(df[i][myidx].startswith('1|0') or df[i][myidx].startswith('0|1') or df[i][myidx].startswith('1/0') or df[i][myidx].startswith('0/1')):\n",
    "                    if(categorical==\"True\"):\n",
    "                        new_df[idx][j] = 1\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([1,1])\n",
    "                        new_df[idx][j][0] = 1\n",
    "                        new_df[idx][j][1] = 1\n",
    "                elif(df[i][myidx].startswith('0|0') or df[i][myidx].startswith('0/0')):\n",
    "                    if(categorical==\"True\"): \n",
    "                        new_df[idx][j] = 0\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([2,0])\n",
    "                        new_df[idx][j][0] = 2\n",
    "                        new_df[idx][j][1] = 0\n",
    "                else:\n",
    "                    if(categorical==\"True\"):\n",
    "                        new_df[idx][j] = -1\n",
    "                    else:\n",
    "                        #new_df[idx][j] = np.array([0,0]) \n",
    "                        new_df[idx][j][0] = 0 \n",
    "                        new_df[idx][j][1] = 0 \n",
    "            else:\n",
    "                if(categorical==\"True\"):\n",
    "                    new_df[idx][j] = -1\n",
    "                else:\n",
    "                    #new_df[idx][j] = np.array([0,0]) \n",
    "                    new_df[idx][j][0] = 0 \n",
    "                    new_df[idx][j][1] = 0 \n",
    "                if(idx==0):\n",
    "                    print(j)\n",
    "                #RR I forgot to mention that we have to take into account possible missing data\n",
    "                #RR in case there is missing data (NA, .|., -|-, or anything different from 0|0, 1|1, 0|1, 1|0) = 3\n",
    "            j += 1\n",
    "        i += 1\n",
    "        #pbar.update(1)\n",
    "        idx += 1\n",
    "\n",
    "    #print(\"processed_data\")\n",
    "    #for i in range(10):\n",
    "    #    print(new_df[i][0])\n",
    "\n",
    "    #the data needs to be flattened because the matrix multiplication step (x*W) \n",
    "    #doesn't support features with subfeatures (matrix of vectors)\n",
    "    #new_df = np.reshape(new_df, (new_df.shape[0],new_df.shape[1]*2))\n",
    "    #print(new_df.shape)\n",
    "    #pbar.close()\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time to load the data (sec): ', stop - start)\n",
    "    \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(mydata, mask_rate=0.9, categorical=\"False\"):\n",
    "    start = timeit.default_timer()\n",
    "    # random matrix the same shape of your data\n",
    "    #print(len(mydata))\n",
    "    nmask = int(round(len(mydata[0])*mask_rate))\n",
    "    # random boolean mask for which values will be changed\n",
    "    maskindex = random.sample(range(0, len(mydata[0]-1)), nmask)\n",
    "    maskindex = [211, 654, 274, 410, 236, 731, 678, 490, 710, 650, 377, 826, 483, 828, 283, 237, 625, 844, 796, 428, 235, 238, 185, 790, 630, 89, 51, 760, 134, 421, 217, 138, 513, 116, 824, 623, 266, 142, 286, 158, 485, 309, 794, 546, 203, 809, 333, 593, 258, 172, 605, 293, 466, 693, 97, 234, 326, 764, 628, 372, 426, 46, 52, 752, 680, 493, 798, 507, 98, 243, 87, 31, 477, 510, 28, 264, 606, 166, 154, 63, 374, 547, 57, 91, 543, 818, 363, 122, 218, 219, 365, 624, 703, 821, 707, 191, 423, 61, 399, 738, 222, 540, 685, 801, 737, 104, 587, 793, 661, 486, 782, 37, 516, 772, 814, 305, 588, 103, 859, 762, 22, 567, 487, 223, 658, 541, 523, 33, 296, 465, 130, 626, 361, 761, 419, 111, 226, 622, 402, 163, 132, 691, 39, 648, 495, 287, 492, 40, 190, 501, 520, 827, 231, 770, 713, 659, 702, 340, 517, 24, 277, 749, 221, 524, 167, 504, 126, 531, 617, 527, 371, 742, 108, 719, 164, 392, 739, 602, 298, 41, 640, 674, 313, 646, 422, 566, 594, 290, 383, 187, 837, 112, 450, 603, 457, 805, 500, 148, 629, 225, 389, 21, 591, 627, 378, 3, 512, 449, 570, 140, 855, 263, 468, 143, 322, 544, 604, 451, 754, 242, 95, 10, 413, 49, 864, 347, 461, 141, 375, 689, 692, 706, 345, 542, 849, 145, 107, 803, 584, 780, 632, 385, 579, 503, 69, 415, 788, 81, 330, 643, 559, 56, 284, 438, 571, 718, 241, 34, 316, 532, 817, 775, 665, 58, 511, 787, 268, 536, 147, 379, 533, 637, 740, 14, 783, 118, 171, 168, 366, 84, 842, 853, 357, 319, 292, 88, 406, 324, 829, 765, 489, 408, 435, 444, 156, 215, 109, 644, 808, 607, 744, 663, 845, 4, 12, 220, 865, 356, 331, 418, 813, 315, 447, 454, 539, 779, 170, 251, 83, 631, 189, 73, 262, 105, 858, 615, 188, 353, 301, 581, 700, 350, 456, 730, 555, 664, 196, 70, 458, 529, 723, 725, 840, 32, 247, 60, 306, 439, 38, 573, 834, 822, 387, 48, 701, 216, 564, 810, 317, 442, 54, 327, 437, 254, 386, 240, 230, 734, 590, 151, 136, 472, 328, 836, 611, 515, 820, 460, 250, 589, 425, 5, 786, 497, 655, 666, 568, 657, 535, 561, 233, 835, 71, 125, 756, 131, 769, 161, 578, 862, 577, 653, 123, 720, 272, 181, 528, 552, 178, 807, 208, 152, 96, 434, 209, 785, 621, 619, 176, 462, 295, 610, 649, 745, 232, 514, 861, 667, 239, 229, 433, 260, 159, 113, 481, 360, 35, 436, 494, 397, 11, 416, 797, 72, 25, 213, 595, 320, 198, 348, 560, 395, 153, 771, 727, 318, 66, 139, 92, 99, 694, 411, 179, 502, 304, 804, 120, 252, 101, 750, 403, 9, 204, 480, 741, 811, 42, 592, 129, 598, 599, 124, 843, 329, 574, 224, 19, 420, 396, 310, 43, 370, 812, 792, 777, 521, 846, 635, 115, 393, 645, 609, 368, 616, 519, 160, 755, 380, 362, 711, 94, 311, 746, 409, 802, 776, 312, 68, 400, 816, 735, 668, 280, 373, 67, 30, 612, 660, 302, 509, 259, 173, 841, 390, 582, 79, 369, 200, 192, 26, 681, 733, 384, 526, 597, 585, 795, 246, 647, 554, 271, 833, 55, 257, 850, 478, 565, 417, 498, 269, 430, 613, 401, 128, 227, 537, 114, 0, 854, 248, 194, 699, 747, 407, 851, 228, 342, 86, 183, 155, 278, 508, 636, 614, 135, 459, 212, 424, 496, 452, 100, 273, 245, 684, 671, 766, 337, 484, 59, 376, 297, 106, 341, 1, 839, 344, 332, 146, 732, 714, 580, 180, 29, 355, 856, 276, 78, 767, 548, 214, 819, 18, 2, 62, 557, 76, 751, 275, 562, 499, 728, 525, 279, 15, 432, 45, 823, 119, 85, 475, 675, 569, 255, 300, 608, 690, 708, 784, 253, 6, 715, 736, 551, 729, 448, 717, 20, 102, 367, 679, 47, 575, 641, 455, 336, 832, 670, 323, 398, 556, 726, 64, 427, 202, 186, 530, 93, 848, 748, 695, 482, 36, 77, 82, 620, 388, 23, 474, 314, 470, 338, 669, 759, 860, 800, 696, 852, 743, 583, 618, 506, 601, 53, 65, 463, 633, 716, 169, 288, 414, 774, 291, 193, 596, 261, 768, 704, 453, 440, 359, 550, 381, 90, 473, 758, 303, 150, 294, 697, 799, 174, 197, 445, 265, 335, 639, 572, 446, 863, 825, 80, 299, 781, 133, 538, 144, 270, 206, 7, 207, 471, 175, 74, 753, 44, 712, 382, 722, 600, 334, 13, 182, 349, 476, 488, 165, 687, 709, 210, 351, 431, 576, 682, 321, 549, 308, 289, 50, 121]\n",
    "    #np.random.randint(0,len(mydata[0]),size=nmask)\n",
    "    print(\"Masking markers...\")\n",
    "    print(maskindex)\n",
    "    #mydata = np.transpose(mydata)\n",
    "    print(mydata.shape)\n",
    "    #mydata\n",
    "    \n",
    "    #pbar = tqdm(total = len(maskindex))\n",
    "    #for i in range(10):\n",
    "    #    print(mydata[i][0:11])\n",
    "\n",
    "\n",
    "    for i in maskindex:\n",
    "        #print(len(mydata[i]))\n",
    "        j = 0\n",
    "        while j < len(mydata):\n",
    "            if(categorical==\"True\"):\n",
    "                mydata[j][i]=-1\n",
    "            else:\n",
    "                mydata[j][i]=[0,0]\n",
    "            j=j+1\n",
    "        #pbar.update(1)\n",
    "        #print(mydata[i])\n",
    "    #mydata = np.transpose(mydata)\n",
    "    #print(mydata.shape)\n",
    "    #pbar.close()\n",
    "\n",
    "    #print(\"after masking:\")\n",
    "    #for i in range(10):\n",
    "    #    print(mydata[i][0:11])\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time to mask the data (sec): ', stop - start)  \n",
    "    return mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(mydata):\n",
    "    #subjects, SNP, REF/ALT counts\n",
    "    if(len(mydata.shape) == 3):\n",
    "        mydata = np.reshape(mydata, (mydata.shape[0],-1))\n",
    "    else:#do one hot encoding, depth=3 because missing (-1) is encoded to all zeroes\n",
    "        mydata = tf.one_hot(indices=mydata, depth=3)\n",
    "        mydata = tf.layers.flatten(mydata)#flattening to simplify calculations later (matmul, add, etc)\n",
    "    return mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = process_data(\"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.pos.clean3\", \"ARIC_PLINK_flagged_chromosomal_abnormalities_zeroed_out_bed.lifted_NCBI36_to_GRCh37.GH.ancestry-1.chr9_intersect1.vcf.gz.9p21.3.recode.vcf\", categorical=\"False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input data.\n",
      "False\n",
      "Time to load the data (sec):  163.51829885796178\n"
     ]
    }
   ],
   "source": [
    "new_df_obs = process_data(\"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.pos.clean3\", \"c1_ARIC_WGS_Freeze3.lifted_already_GRCh37_intersect1.vcf.gz.9p21.3.recode.vcf\", categorical=\"False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_new_df_obs = np.copy(new_df_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking markers...\n",
      "[211, 654, 274, 410, 236, 731, 678, 490, 710, 650, 377, 826, 483, 828, 283, 237, 625, 844, 796, 428, 235, 238, 185, 790, 630, 89, 51, 760, 134, 421, 217, 138, 513, 116, 824, 623, 266, 142, 286, 158, 485, 309, 794, 546, 203, 809, 333, 593, 258, 172, 605, 293, 466, 693, 97, 234, 326, 764, 628, 372, 426, 46, 52, 752, 680, 493, 798, 507, 98, 243, 87, 31, 477, 510, 28, 264, 606, 166, 154, 63, 374, 547, 57, 91, 543, 818, 363, 122, 218, 219, 365, 624, 703, 821, 707, 191, 423, 61, 399, 738, 222, 540, 685, 801, 737, 104, 587, 793, 661, 486, 782, 37, 516, 772, 814, 305, 588, 103, 859, 762, 22, 567, 487, 223, 658, 541, 523, 33, 296, 465, 130, 626, 361, 761, 419, 111, 226, 622, 402, 163, 132, 691, 39, 648, 495, 287, 492, 40, 190, 501, 520, 827, 231, 770, 713, 659, 702, 340, 517, 24, 277, 749, 221, 524, 167, 504, 126, 531, 617, 527, 371, 742, 108, 719, 164, 392, 739, 602, 298, 41, 640, 674, 313, 646, 422, 566, 594, 290, 383, 187, 837, 112, 450, 603, 457, 805, 500, 148, 629, 225, 389, 21, 591, 627, 378, 3, 512, 449, 570, 140, 855, 263, 468, 143, 322, 544, 604, 451, 754, 242, 95, 10, 413, 49, 864, 347, 461, 141, 375, 689, 692, 706, 345, 542, 849, 145, 107, 803, 584, 780, 632, 385, 579, 503, 69, 415, 788, 81, 330, 643, 559, 56, 284, 438, 571, 718, 241, 34, 316, 532, 817, 775, 665, 58, 511, 787, 268, 536, 147, 379, 533, 637, 740, 14, 783, 118, 171, 168, 366, 84, 842, 853, 357, 319, 292, 88, 406, 324, 829, 765, 489, 408, 435, 444, 156, 215, 109, 644, 808, 607, 744, 663, 845, 4, 12, 220, 865, 356, 331, 418, 813, 315, 447, 454, 539, 779, 170, 251, 83, 631, 189, 73, 262, 105, 858, 615, 188, 353, 301, 581, 700, 350, 456, 730, 555, 664, 196, 70, 458, 529, 723, 725, 840, 32, 247, 60, 306, 439, 38, 573, 834, 822, 387, 48, 701, 216, 564, 810, 317, 442, 54, 327, 437, 254, 386, 240, 230, 734, 590, 151, 136, 472, 328, 836, 611, 515, 820, 460, 250, 589, 425, 5, 786, 497, 655, 666, 568, 657, 535, 561, 233, 835, 71, 125, 756, 131, 769, 161, 578, 862, 577, 653, 123, 720, 272, 181, 528, 552, 178, 807, 208, 152, 96, 434, 209, 785, 621, 619, 176, 462, 295, 610, 649, 745, 232, 514, 861, 667, 239, 229, 433, 260, 159, 113, 481, 360, 35, 436, 494, 397, 11, 416, 797, 72, 25, 213, 595, 320, 198, 348, 560, 395, 153, 771, 727, 318, 66, 139, 92, 99, 694, 411, 179, 502, 304, 804, 120, 252, 101, 750, 403, 9, 204, 480, 741, 811, 42, 592, 129, 598, 599, 124, 843, 329, 574, 224, 19, 420, 396, 310, 43, 370, 812, 792, 777, 521, 846, 635, 115, 393, 645, 609, 368, 616, 519, 160, 755, 380, 362, 711, 94, 311, 746, 409, 802, 776, 312, 68, 400, 816, 735, 668, 280, 373, 67, 30, 612, 660, 302, 509, 259, 173, 841, 390, 582, 79, 369, 200, 192, 26, 681, 733, 384, 526, 597, 585, 795, 246, 647, 554, 271, 833, 55, 257, 850, 478, 565, 417, 498, 269, 430, 613, 401, 128, 227, 537, 114, 0, 854, 248, 194, 699, 747, 407, 851, 228, 342, 86, 183, 155, 278, 508, 636, 614, 135, 459, 212, 424, 496, 452, 100, 273, 245, 684, 671, 766, 337, 484, 59, 376, 297, 106, 341, 1, 839, 344, 332, 146, 732, 714, 580, 180, 29, 355, 856, 276, 78, 767, 548, 214, 819, 18, 2, 62, 557, 76, 751, 275, 562, 499, 728, 525, 279, 15, 432, 45, 823, 119, 85, 475, 675, 569, 255, 300, 608, 690, 708, 784, 253, 6, 715, 736, 551, 729, 448, 717, 20, 102, 367, 679, 47, 575, 641, 455, 336, 832, 670, 323, 398, 556, 726, 64, 427, 202, 186, 530, 93, 848, 748, 695, 482, 36, 77, 82, 620, 388, 23, 474, 314, 470, 338, 669, 759, 860, 800, 696, 852, 743, 583, 618, 506, 601, 53, 65, 463, 633, 716, 169, 288, 414, 774, 291, 193, 596, 261, 768, 704, 453, 440, 359, 550, 381, 90, 473, 758, 303, 150, 294, 697, 799, 174, 197, 445, 265, 335, 639, 572, 446, 863, 825, 80, 299, 781, 133, 538, 144, 270, 206, 7, 207, 471, 175, 74, 753, 44, 712, 382, 722, 600, 334, 13, 182, 349, 476, 488, 165, 687, 709, 210, 351, 431, 576, 682, 321, 549, 308, 289, 50, 121]\n",
      "(1456, 866, 2)\n",
      "Time to mask the data (sec):  1.8043172999750823\n"
     ]
    }
   ],
   "source": [
    "new_df = mask_data(np.copy(orig_new_df_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input data.\n",
      "False\n",
      "Time to load the data (sec):  3491.9932977700373\n"
     ]
    }
   ],
   "source": [
    "new_df_obs2 = process_data(\"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.pos.clean3\", \"HRC.r1-1.EGA.GRCh37.chr9.haplotypes.9p21.3.vcf.clean3\", categorical=\"False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = flatten(new_df.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 1732)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_obs = flatten(new_df_obs.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 1732)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_df_obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_obs2 = flatten(new_df_obs2.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27165, 1732)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_df_obs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define layer size\n",
    "#n_input = len(new_df[0])     # input features N_variants\n",
    "#n_hidden_1 = n_input  # hidden layer for encoder, equal to input number of features for now\n",
    "#print(n_input)\n",
    "#tf input\n",
    "#X = tf.placeholder(\"float\", [None, n_input])\n",
    "#Y = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "    \n",
    "#biases = {\n",
    "#    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#    'decoder_b1': tf.Variable(tf.random_normal([n_input])),\n",
    "#}\n",
    "\n",
    "    #print(X.get_shape())\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=False)\n",
    "config.intra_op_parallelism_threads = 4\n",
    "config.inter_op_parallelism_threads = 4\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.15\n",
    "#config.gpu_options.allow_growth=True\n",
    "#with tf.device('/device:GPU:0'):  # Replace with device you are interested in\n",
    "bytes_in_use = BytesInUse()\n",
    "    \n",
    "sess=tf.Session(config=config) \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    #saver.restore(sess,tf.train.latest_checkpoint('/home/rdias/myscripts/raqueld/Autoencoder_tensorflow/10-fold_CV_F_new_backup2/'))\n",
    "    saver = tf.train.import_meta_graph('./10-fold_CV_F_new_backup2/inference_model-3.ckpt.meta')\n",
    "\n",
    "    saver.restore(sess,'./10-fold_CV_F_new_backup2/inference_model-3.ckpt')\n",
    "    #with tf.device('/device:GPU:0'): \n",
    "    # Access saved Variables directly\n",
    "    print(sess.run('Y:0', feed_dict={\"X:0\": new_df, \"Y:0\": new_df_obs}))\n",
    "    print(\"\\n****\\n\")\n",
    "    print(new_df_obs)\n",
    "    print(\"\\n****\\n\")\n",
    "    print(sess.run('X:0', feed_dict={\"X:0\": new_df, \"Y:0\": new_df_obs}))\n",
    "    print(\"\\n****\\n\")\n",
    "    print(new_df)\n",
    "    y_pred = (sess.run('y_pred:0', feed_dict={\"X:0\": new_df, \"Y:0\": new_df_obs}))\n",
    "    print(\"\\n****\\n\")\n",
    "    print(y_pred[0:10])\n",
    "    \n",
    "    j=0\n",
    "    while j < (len(y_pred[0])-1):\n",
    "        test1 = np.zeros(shape=(len(y_pred),2))\n",
    "        test2 = np.zeros(shape=(len(y_pred),2))\n",
    "        for i in range(len(y_pred)):\n",
    "            test1[i][0] = y_pred[i][j]\n",
    "            test2[i][0] =  new_df_obs[i][j]\n",
    "            j +=1\n",
    "            test1[i][1] = y_pred[i][j]\n",
    "            test2[i][1] =  new_df_obs[i][j]       \n",
    "        \n",
    "        correct_prediction = sess.run( tf.equal( tf.round( test1 ), tf.round( test2 ) ) )\n",
    "        accuracy = sess.run(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n",
    "        print(accuracy)\n",
    "        j += 1\n",
    "        \n",
    "        \n",
    "    correct_prediction = sess.run( tf.equal( tf.round( tf.cast(y_pred, tf.float64) ), tf.round( new_df_obs ) ) )\n",
    "\n",
    "            \n",
    "    correct_prediction = sess.run( tf.equal( tf.round( tf.cast(y_pred, tf.float64) ), tf.round( new_df_obs ) ) )\n",
    "    accuracy = sess.run(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76674366\n"
     ]
    }
   ],
   "source": [
    "#chkp.print_tensors_in_checkpoint_file(\"/tmp/model.ckpt\", tensor_name='v2', all_tensors=False)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7695012\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = sess.run( tf.equal( tf.round( tf.cast(y_pred, 'float64') ), tf.round( new_df_obs ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True, False, False])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "    accuracy = sess.run(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36719286\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph = sess.graph\n",
    "#print([node.name for node in graph.as_graph_def().node])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36719286\n"
     ]
    }
   ],
   "source": [
    "print(sess.run('accuracy:0', feed_dict={\"X:0\": new_df, \"Y:0\": new_df_obs}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chkp.print_tensors_in_checkpoint_file(\"/tmp/model.ckpt\", tensor_name='', all_tensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
